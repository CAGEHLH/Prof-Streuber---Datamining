{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"Module5_MeasuresOfAssociation.ipynb","provenance":[],"collapsed_sections":["bZu4GZJ5n0ZO"],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"bZu4GZJ5n0ZO"},"source":["# **Module 5: Measures of Association**\n","\n","As we have already seen, evaluating the relationship between variables is at the heart of data mining. We have a number of techniques at our disposal:\n","1. Distance calculations like Euclidian, Manhattan, Cosine Similarity (and some others)\n","2. Measures of Independence like Chi Square, Covariance, and Correlation\n","3. Predictive Measures like Regression and ANOVA\n","\n","In this module, you will learn how to:\n","* Explain distance between data in terms of Chi Square, Correlation, and Cosine Similarity\n","* Set up Cosine Similarity calculations\n","* Set up Chi Square calculations\n","* Set up Correlation calculations"]},{"cell_type":"markdown","metadata":{"id":"SX1NUuGsnXJJ"},"source":["#**0. Preparation and Setup**\n","We are working with our adult dataset again, so we're loading our libraries and our dataset just like last time. Since we are working with more involved math this time around, we will need more functionality:\n","* **Scikit-Learn**: We will use the cosine_similarity function from the [scikit-learn metrics module](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics). If you click that link, you'll see that this is the big math module--knowing which functions to use will allow you to skip programming much of the math by hand. Very practical! Cosine similarity is located in the [pairwise submodule](https://scikit-learn.org/stable/modules/metrics.html#metrics).\n","* **Scipy**: This is numpy's [\"big sister\" for advanced math](https://docs.scipy.org/doc/scipy-1.6.3/reference/). We will use it for Chi Square calculations.\n","* **Math**: The [math package](https://docs.python.org/3/library/math.html) allows us to do advanced math, calculate logarithmic functions etc."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":197},"id":"rx3vL3ekndfA","executionInfo":{"status":"ok","timestamp":1623112735714,"user_tz":300,"elapsed":599,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"c4e8c9b4-b880-4a65-a97b-8c0b5bfec93c"},"source":["import numpy as np\n","import pandas as pd \n","import matplotlib.pyplot as plt\n","import math\n","from math import acos, degrees\n","from scipy import spatial\n","from scipy.stats import chisquare\n","from scipy.stats import chi2_contingency\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","from IPython.display import HTML # This is just for me so I can embed videos\n","from IPython.display import Image # This is just for me so I can embed images\n","\n","#Reading in the data as adult dataframe\n","adult = pd.read_csv(\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/data/adult.data.simplified.csv\")\n","\n","#Verifying that we can see the data\n","adult.head()"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>workclass</th>\n","      <th>education</th>\n","      <th>educationyears</th>\n","      <th>maritalstatus</th>\n","      <th>occupation</th>\n","      <th>relationship</th>\n","      <th>race</th>\n","      <th>sex</th>\n","      <th>hoursperweek</th>\n","      <th>nativecountry</th>\n","      <th>incomeUSD</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>39</td>\n","      <td>State-gov</td>\n","      <td>Bachelors</td>\n","      <td>13</td>\n","      <td>Never-married</td>\n","      <td>Adm-clerical</td>\n","      <td>Not-in-family</td>\n","      <td>White</td>\n","      <td>Male</td>\n","      <td>40</td>\n","      <td>United-States</td>\n","      <td>43747</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>50</td>\n","      <td>Self-emp-not-inc</td>\n","      <td>Bachelors</td>\n","      <td>13</td>\n","      <td>Married-civ-spouse</td>\n","      <td>Exec-managerial</td>\n","      <td>Husband</td>\n","      <td>White</td>\n","      <td>Male</td>\n","      <td>13</td>\n","      <td>United-States</td>\n","      <td>38907</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>38</td>\n","      <td>Private</td>\n","      <td>HS-grad</td>\n","      <td>9</td>\n","      <td>Divorced</td>\n","      <td>Handlers-cleaners</td>\n","      <td>Not-in-family</td>\n","      <td>White</td>\n","      <td>Male</td>\n","      <td>40</td>\n","      <td>United-States</td>\n","      <td>25055</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>53</td>\n","      <td>Private</td>\n","      <td>11th</td>\n","      <td>7</td>\n","      <td>Married-civ-spouse</td>\n","      <td>Handlers-cleaners</td>\n","      <td>Husband</td>\n","      <td>Black</td>\n","      <td>Male</td>\n","      <td>40</td>\n","      <td>United-States</td>\n","      <td>26733</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>28</td>\n","      <td>Private</td>\n","      <td>Bachelors</td>\n","      <td>13</td>\n","      <td>Married-civ-spouse</td>\n","      <td>Prof-specialty</td>\n","      <td>Wife</td>\n","      <td>Black</td>\n","      <td>Female</td>\n","      <td>40</td>\n","      <td>Cuba</td>\n","      <td>23429</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   age         workclass  education  ...  hoursperweek  nativecountry incomeUSD\n","0   39         State-gov  Bachelors  ...            40  United-States     43747\n","1   50  Self-emp-not-inc  Bachelors  ...            13  United-States     38907\n","2   38           Private    HS-grad  ...            40  United-States     25055\n","3   53           Private       11th  ...            40  United-States     26733\n","4   28           Private  Bachelors  ...            40           Cuba     23429\n","\n","[5 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"aljcjl3kn0ZX"},"source":["# **1. Cosine Similarity**\n","\n","Cosine Similarity is the last similarity measure that we haven't discussed yet (we've already talked about Euclidean and Manhattan Distance). \n","\n","Here is a graphical overview of all the distance measures:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":445},"id":"h9b4mHDWsSJv","executionInfo":{"status":"ok","timestamp":1623112736680,"user_tz":300,"elapsed":383,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"568a8d05-8f70-4b11-f62b-a5c353874820"},"source":["Image(url='https://i0.wp.com/dataaspirant.com/wp-content/uploads/2015/04/cover_post_final.png') "],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"https://i0.wp.com/dataaspirant.com/wp-content/uploads/2015/04/cover_post_final.png\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"GzMX3PSXszAp"},"source":["As you can see, while Euclidean and Manhattan Distance are just plain calculations, Cosine Similarity works with angles. This is where things get a little more involved. \n","\n","Mathematically speaking, **Cosine Similarity** is the normalized dot product between two vectors, i.e. two attributes in our dataframe. The steps are\n","* Calculate the dot product between two attributes (or, in math language: Calculate the product of the Euclidean magnitudes of these two vectors). If you're not sure what dot product is, [here is a great explanation](https://www.mathsisfun.com/algebra/vectors-dot-product.html).\n","* Calculate the cosine of the angle between these two vectors.\n","Quick helper: The cosine of 0° is 1 (meaning the two vectors are identical), and it is less than 1 for any other angle. It is 0 for a 90° angle.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":555},"id":"V4EvLD7vxH9m","executionInfo":{"status":"ok","timestamp":1623112736688,"user_tz":300,"elapsed":349,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"5e395f87-6f13-4686-8831-48acb6e7bb47"},"source":["Image(url='https://i0.wp.com/dataaspirant.com/wp-content/uploads/2015/04/cosine.png') "],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"https://i0.wp.com/dataaspirant.com/wp-content/uploads/2015/04/cosine.png\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"QHGCvSJMxgMp"},"source":["Since Cosine Similarity can be computed relatively quickly, it is very popular.\n","\n","More information is here: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html"]},{"cell_type":"markdown","metadata":{"id":"9B4xiSPMwTXf"},"source":["## **1.1 A Small Example**\n","Here is how this works on a very small example, with two made-up vectors.\n","Note that all the vectors **MUST be numeric**! (One big reason why we  covered preprocessing techniques in the previous module)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SZ-AUVlZyVEC","executionInfo":{"status":"ok","timestamp":1623112736706,"user_tz":300,"elapsed":332,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"5005ba37-0cdc-4e1a-b9dc-2aad2a9ae38f"},"source":["# Here are the two vectors. THink of them like numeric attributes from your dataframe:\n","a = np.array([1,2,3])\n","b = np.array([1,1,4])\n","\n","print(a,b)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[1 2 3] [1 1 4]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RkHKpDQ-5SXG"},"source":["First, we do this manually:"]},{"cell_type":"code","metadata":{"id":"l_-EbCt_n0ZY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623112736715,"user_tz":300,"elapsed":306,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"d6d2cc35-39ec-4132-f123-adfb484e1f23"},"source":["# We can MANUALLY compute cosine similarity (this is where we are using the math library we called above)\n","dot = np.dot(a, b)  # computing the dot product with the dot function\n","norma = np.linalg.norm(a)  # normalizing vector a\n","normb = np.linalg.norm(b)  # normalizing vector b\n","cos1 = dot / (norma * normb)  # computing the cosine by dividing the dot product by the normalized vectors as shown in the cartoon above\n","\n","print(cos1)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["0.9449111825230682\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ynXFZ3N72j81","executionInfo":{"status":"ok","timestamp":1623112736723,"user_tz":300,"elapsed":280,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"192546a4-c8a0-4c6b-d25e-707d398eaf31"},"source":["# Now we need to find the angle from the cosine. This requires two steps: \n","# 1. convert the angle to radians\n","# 2. calculate the degrees\n","\n","angle_in_radians1 = math.acos(cos1)\n","degrees1 = math.degrees(angle_in_radians1)\n","\n","print(cos1, angle_in_radians1, degrees1)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["0.9449111825230682 0.3334731722518318 19.106605350869078\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YlBnZAzo5bGa"},"source":["So, that was straight-up programmed math. \n","\n","But this is data mining, and we have fancy packages like sklearn.metrics to speed things up. So, we'll do this over again:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":384},"id":"juT_ZNgP6ons","executionInfo":{"status":"error","timestamp":1623112736965,"user_tz":300,"elapsed":484,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"9f99553c-349b-4bd6-9e96-b01538e91ade"},"source":["# Using the COSINE_SIMILARITY FUNCTION from sklearn.metrics\n","cos2 = cosine_similarity(a,b)"],"execution_count":8,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-4a7c2aa4239e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Using the COSINE_SIMILARITY FUNCTION from sklearn.metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcos2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[0;31m# to avoid recursive import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    139\u001b[0m         X = check_array(X, accept_sparse=accept_sparse, dtype=dtype,\n\u001b[1;32m    140\u001b[0m                         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                         estimator=estimator)\n\u001b[0m\u001b[1;32m    142\u001b[0m         Y = check_array(Y, accept_sparse=accept_sparse, dtype=dtype,\n\u001b[1;32m    143\u001b[0m                         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    554\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1. 2. 3.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."]}]},{"cell_type":"markdown","metadata":{"id":"zD5Xixr76ybP"},"source":["**Wait, what?** Let's **read the error message** here: It tells us that something is wrong with the way our input data is formatted. In fact, the cosine_similarity function expects two-dimensional arrays. Our input data has just one each. That's why we need to reformat (=reshape) our input data. Let's try this again:"]},{"cell_type":"code","metadata":{"id":"F6sgiBimyO2-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623112755000,"user_tz":300,"elapsed":321,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"804e3466-58bd-4f66-e3d8-d6bdeb14880c"},"source":["# Using the COSINE_SIMILARITY FUNCTION from sklearn.metrics\n","aa = a.reshape(1,3) # we have 3 values in our array\n","ba = b.reshape(1,3) # we have 3 values in our array\n","cos3 = cosine_similarity(aa, ba)\n","\n","# Now we need to convert the angle to radians to get the degrees\n","angle_in_radians3 = math.acos(cos3)\n","degrees3 = math.degrees(angle_in_radians3)\n"," \n","print(cos3, angle_in_radians3, degrees3)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[[0.94491118]] 0.3334731722518318 19.106605350869078\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4G4q_k94upkM","executionInfo":{"status":"aborted","timestamp":1623112736738,"user_tz":300,"elapsed":242,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gtYwYBWO9QD7"},"source":["As you can see, both, the manual calculation and the calculation with  scikit-learn get us identical results. On to bigger and better things!"]},{"cell_type":"markdown","metadata":{"id":"zfu2pP8-9ga9"},"source":["##**1.2 Cosine Similarity with the adult Dataset**\n","Now we are going to apply what we have learned above to the adult dataset. Remember that the inputs MUST be numeric!"]},{"cell_type":"code","metadata":{"id":"Cn_z8WTwn0Zb","executionInfo":{"status":"aborted","timestamp":1623112736744,"user_tz":300,"elapsed":247,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}}},"source":["# First, we convert two attributes to numpy arrays\n","a = np.array(adult.educationyears)\n","b = np.array(adult.incomeUSD)\n","\n","# Manually computing cosine similarity\n","dot = np.dot(a, b)  # the dot product\n","norma = np.linalg.norm(a)  # normalizing vector a\n","normb = np.linalg.norm(b)  # normalizing vector b\n","cos1 = dot / (norma * normb)  # computing the angle by dividing the dot product by the normalized vectors\n","\n","print(cos1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"quKt-6zl-NsC"},"source":["## Your Turn\n","1. Compute the angle in degrees from the cos1 value shown above. Use the code field below."]},{"cell_type":"code","metadata":{"id":"LYH3NEHe-eP9","executionInfo":{"status":"aborted","timestamp":1623112736757,"user_tz":300,"elapsed":255,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c_dsjHdd-iyY"},"source":["2. Now, use sklearn.metrics to compute the angle between adult.educationyears and adult.incomeUSD.\n","\n","Remember to reshape the input data. Each vector has 32561 values."]},{"cell_type":"code","metadata":{"id":"PtU85SO19_oz","executionInfo":{"status":"aborted","timestamp":1623112736772,"user_tz":300,"elapsed":268,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MN9-my85n0Ze","executionInfo":{"status":"aborted","timestamp":1623112736779,"user_tz":300,"elapsed":273,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}}},"source":["# Here is a solution that uses the spatial package from scipy\n","\n","a = np.array(adult.educationyears)\n","b = np.array(adult.incomeUSD)\n","\n","result = 1 - spatial.distance.cosine(a, b)\n","\n","# Now we need to convert the angle to radians to get the degrees\n","angle_in_radians = math.acos(result)\n","degrees = math.degrees(angle_in_radians)\n","\n","print(result, degrees)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DhBj3qNfn0Zj"},"source":["#**2. Chi Square**\n","We just learned that Cosine Similarity explains the relationship between numerical attributes or vectors. But what about the other BIG data type--categorical data? That is where Chi Square comes in.\n","\n","To understand how Chi Square works, let's review **hypothesis testing** right quick: You may remember that all hypothesis testing starts by setting up an H0 (null hypothesis) and an Ha (alternative hypothesis). The null hypothesis states whether you assume that the two variables to compare are either dependent or independent (or equal, depending on the math that follows). Then, we use complicated math that results in a p-value. If this p-value is > 0.05 (5%), then your H0 is most likely true; if it is < 0.05 (5%), then your H0 is potentially wrong and Ha is more likely true.\n","\n","Click [this link](https://www.mathsisfun.com/data/chi-square-test.html) to see a great explanation.\n","\n","**THINGS TO REMEMBER** about a Chi Square test:\n","1. Is a hypothesis test based on categorical attributes.\n","2. Uses as its H0 that the two variables under investigation are independent\n","3. Uses a chi square table\n","3. If the resulting p-value is > 0.05, both variables are independent\n","4. If the resulting p-value is < 0.05, both variables are dependent\n","\n","Soooo ... ever wondered whether there was a connection between the color of a character's uniform and their survival rate on Star Trek's U.S.S. Enterprise? Wonder no more, but [check out the Chi Square calculation right here](https://statisticsbyjim.com/hypothesis-testing/chi-square-test-independence-example/).\n"]},{"cell_type":"markdown","metadata":{"id":"hQaq8s8Fn0Zk"},"source":["**In terms of programming**, the most convenient, way is [scipy.stats.chi2_contingency](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html). chi2_contingency automatically computes the expected values from the chi square table, so you don't have to. The entire output can be captured in 4 variables: **chi square number, p-value, degrees of freedom and the array itself.** chi2_contingency works with n-dimensional contingency tables. This is what we need!"]},{"cell_type":"code","metadata":{"id":"O8bM1wgln0Zl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623114041902,"user_tz":300,"elapsed":211,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}},"outputId":"90ef69fe-172c-49eb-db90-b767ede5d6de"},"source":["# Note that pd.crosstab establishes the two-way table\n","# We can even use a print command to format the output:\n","chi2, p, dof, expected = chi2_contingency((pd.crosstab(adult.education, adult.occupation).values))\n","print (f'Chi-square Statistic: {chi2} ,p-value: {p}, Degrees of Freedom: {dof}')"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Chi-square Statistic: 15997.777225542155 ,p-value: 0.0, Degrees of Freedom: 210\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8Q0_rPKnn0Zp"},"source":["# **3. Covariance and Correlation**\n","Covariance is, essentially, the little brother of correlation. The goal for both calculations/ algorithms (about time we used that term, isn't it?) is to determine if two variables are tightly enough connected that we can detect a pattern to their changes. In other words, we want to see if, when one variable changes, the other variable changes as well, in a predictable fashion. \n","\n","More about Correlation and Covariance in the video below\n"]},{"cell_type":"code","metadata":{"id":"dN_3QPijvcDo","executionInfo":{"status":"aborted","timestamp":1623112736799,"user_tz":300,"elapsed":290,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}}},"source":["HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/mwZbt11azGo\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"THFHGWfom3ko"},"source":["##**3.1 Covariance**\n","Covariance indicates the relationship of two variables whenever one variable changes. If an **increase** in one variable results in an **increase** in the other variable, both variables are said to have a positive covariance (the same is two if both variables decrease), which show up as a positive number. If an increase in one variable causes a decrease in the other, that is a negative covariance, i.e. an inverse relationships, which is indicated by a negative number.\n","\n","Since covariance uses only numeric attributes, we can get np.cov() to work easily with educationyears and incomeUSD. For that purpose, we will use np.cov(). \n","\n","Note that, by default np.cov calculates the **SAMPLE** covariance. To get the **POPULATION** covariance, I had to set the Degrees of Freedom to 0. Or, as shown below, you can set bias to True. If you omit bias, you will calculate the SAMPLE variance."]},{"cell_type":"code","metadata":{"id":"Up724smTn0Zp","executionInfo":{"status":"aborted","timestamp":1623112736819,"user_tz":300,"elapsed":308,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}}},"source":["# SAMPLE covariance assumes that degrees of freedom, i.e. ddof=1\n","np.cov([adult.educationyears],[adult.incomeUSD])[0][1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U2lxA2Y2n0Zs","executionInfo":{"status":"aborted","timestamp":1623112736824,"user_tz":300,"elapsed":311,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}}},"source":["# POPULATION covariance can be calculated by setting ddof=0\n","np.cov([adult.educationyears],[adult.incomeUSD], ddof=0)[0][1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4KFfvVBQn0Zv","executionInfo":{"status":"aborted","timestamp":1623112736848,"user_tz":300,"elapsed":330,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}}},"source":["# POPULATION covariance with bias=True setting\n","np.cov([adult.educationyears],[adult.incomeUSD], bias=True)[0][1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yqNvuNWJn0Z0","executionInfo":{"status":"aborted","timestamp":1623112736854,"user_tz":300,"elapsed":328,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}}},"source":["# SAMPLE covariance omitting bias=True setting\n","np.cov([adult.educationyears],[adult.incomeUSD])[0][1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jM4LisuT5FKB"},"source":["So, what do all these numbers mean? \n","\n","Take the last one here: If the years of education increase by 1, the income in USD increases by $33,078. As you can see, the bias setting does not make much of a difference with smaller numbers. With larger numbers, that is a very different ballgame."]},{"cell_type":"markdown","metadata":{"id":"1TJ68YaJ635T"},"source":["## Your Turn\n","Now, calculate the covariance between age and incomeUSD for the POPULATION (remember the bias setting!). Use the code line below."]},{"cell_type":"code","metadata":{"id":"j-xzhIxr7LTh","executionInfo":{"status":"aborted","timestamp":1623112736891,"user_tz":300,"elapsed":363,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Qp6nTuJn0Z2"},"source":["## **3.2 Correlation**\n","\n","As with Chi Square and Covariance, there are numerous ways to calculate the Pearson Correlation Coefficient. The most efficient and elegant way to do this is using the corr() function in pandas. This function can also handle multiple correlations and will, in fact, produce a table with the R-squared values. NOTE that all inputs must be numeric!\n","\n","If correlation (i.e. the Pearson Correlation Coefficient) is between -1 and 0, then the attributes are negatively correlated (as one grows, another one falls). If correlation is between 0 and +1, then the attributes are positively correlated (both move in the same direction). The correlation coefficient is typically called r.\n","\n","Check out the graphic below to see what the different values of r look like."]},{"cell_type":"code","metadata":{"id":"7EX1rRzGJI3H","executionInfo":{"status":"aborted","timestamp":1623112736898,"user_tz":300,"elapsed":368,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}}},"source":["Image(url='https://media.nagwa.com/359143010784/en/thumbnail_l.jpeg')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4gVAtDLyKJT1"},"source":["Now on to our task!"]},{"cell_type":"code","metadata":{"id":"erJ8ISQNn0Z3","executionInfo":{"status":"aborted","timestamp":1623112736904,"user_tz":300,"elapsed":372,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}}},"source":["# We first want to see if educationyears and incomeUSD are correlated. So, we pass both arrays into the function:\n","corr_edUSD = adult['educationyears'].corr(adult['incomeUSD'])\n","corr_edUSD"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yEtEFd1cn0Z5","executionInfo":{"status":"aborted","timestamp":1623112736931,"user_tz":300,"elapsed":396,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}}},"source":["# We can now plot this with pyplot\n","plt.scatter(adult['educationyears'], adult['incomeUSD'], alpha=0.5)\n","plt.title('Scatter plot educationyears vs. incomeUSD')\n","plt.xlabel('educationyears')\n","plt.ylabel('incomeUSD')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QdE_xhAkn0Z7","executionInfo":{"status":"aborted","timestamp":1623112736944,"user_tz":300,"elapsed":407,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}}},"source":["# The power of the corr() function lies in its ability to handle multiple dimensions at the same time:\n","corr = adult.corr()\n","corr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pIsXk8H_n0Z_","executionInfo":{"status":"aborted","timestamp":1623112736948,"user_tz":300,"elapsed":408,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}}},"source":["# Now we can make a pretty correlation heatmap with pyplot\n","fig = plt.figure()\n","ax = fig.add_subplot(111)\n","cax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)\n","fig.colorbar(cax)\n","ticks = np.arange(0,len(corr.columns),1)\n","ax.set_xticks(ticks)\n","plt.xticks(rotation=90)\n","ax.set_yticks(ticks)\n","ax.set_xticklabels(corr.columns)\n","ax.set_yticklabels(corr.columns)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uyJGWLzFn0aC"},"source":["In the examples above, we see the most frequently used Pearson Correlation in action. The corr() function allows us to work with other kinds of correlations, as well, including the Spearman correlation. \n","For some great examples, take a look at this website: https://www.datascience.com/learn-data-science/fundamentals/introduction-to-correlation-python-data-science"]},{"cell_type":"markdown","metadata":{"id":"_H2wvPHOKb6v"},"source":["##Your Turn\n","Now, calculate the following:\n","1. The COVARIANCE between hoursperweek and incomeUSD"]},{"cell_type":"code","metadata":{"id":"gHA-KvfRKrac","executionInfo":{"status":"aborted","timestamp":1623112736956,"user_tz":300,"elapsed":411,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pCIaMU87Kr20"},"source":["2. The CORRELATION between hoursperweek and incomeUSD"]},{"cell_type":"code","metadata":{"id":"7zd35pTpKwGI","executionInfo":{"status":"aborted","timestamp":1623112736960,"user_tz":300,"elapsed":412,"user":{"displayName":"Sonja Streuber","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQD93L4k89S61xRuTv6_J_-STpROkvxgQny5nHWQ=s64","userId":"07600325844086946297"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s1mOvWU2Kwdk"},"source":["Can you explain how the two numbers are related?"]}]}